"""
GitHub API ä¸‹è½½å™¨æ¨¡å—

ä½¿ç”¨ GitHub REST API è·å–ä»“åº“æ–‡ä»¶æ ‘ï¼Œç­›é€‰å¹¶ä¸‹è½½ txt/md/docx æ–‡ä»¶ã€‚
"""

import base64
import fnmatch
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Optional
from urllib.request import Request, urlopen
from urllib.error import HTTPError
from urllib.parse import quote
import json

from .config_manager import Config


@dataclass
class FileItem:
    """æ–‡ä»¶æ¡ç›®"""
    path: str           # ç›¸å¯¹äºä»“åº“æ ¹ç›®å½•çš„å®Œæ•´è·¯å¾„
    name: str           # æ–‡ä»¶å
    sha: str            # Git SHA
    download_url: str   # ä¸‹è½½åœ°å€
    size: int           # æ–‡ä»¶å¤§å° (bytes)


# æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
SUPPORTED_EXTENSIONS = {".txt", ".md", ".docx"}


def _make_request(url: str, token: Optional[str] = None) -> dict:
    """å‘é€ GitHub API è¯·æ±‚"""
    headers = {
        "Accept": "application/vnd.github.v3+json",
        "User-Agent": "prolet-tools/1.0",
    }
    if token:
        headers["Authorization"] = f"Bearer {token}"

    req = Request(url, headers=headers)
    try:
        with urlopen(req, timeout=30) as resp:
            return json.loads(resp.read().decode("utf-8"))
    except HTTPError as e:
        if e.code == 403:
            raise RuntimeError(f"GitHub API é™æµæˆ–æ— æƒé™: {e.reason}")
        elif e.code == 404:
            raise RuntimeError(f"ä»“åº“æˆ–è·¯å¾„ä¸å­˜åœ¨: {url}")
        raise


def _fetch_tree_recursive(
    owner: str,
    repo: str,
    branch: str,
    token: Optional[str],
    exclude_patterns: list[str],
    exclude_files: list[str],
) -> list[FileItem]:
    """
    ä½¿ç”¨ Git Database API é€’å½’è·å–æ•´ä¸ªæ–‡ä»¶æ ‘

    è¿”å›ç­›é€‰åçš„æ–‡ä»¶åˆ—è¡¨
    """
    # è·å– branch æœ€æ–° commit çš„ tree SHA
    ref_url = f"https://api.github.com/repos/{owner}/{repo}/git/ref/heads/{branch}"
    ref_data = _make_request(ref_url, token)
    commit_sha = ref_data["object"]["sha"]

    # è·å– commit å¯¹åº”çš„ tree
    commit_url = f"https://api.github.com/repos/{owner}/{repo}/git/commits/{commit_sha}"
    commit_data = _make_request(commit_url, token)
    tree_sha = commit_data["tree"]["sha"]

    # é€’å½’è·å–æ•´ä¸ª tree
    tree_url = f"https://api.github.com/repos/{owner}/{repo}/git/trees/{tree_sha}?recursive=1"
    tree_data = _make_request(tree_url, token)

    files: list[FileItem] = []

    for item in tree_data.get("tree", []):
        if item["type"] != "blob":
            continue

        path = item["path"]
        name = Path(path).name
        ext = Path(path).suffix.lower()

        # æ£€æŸ¥æ‰©å±•å
        if ext not in SUPPORTED_EXTENSIONS:
            continue

        # æ£€æŸ¥æ’é™¤æ–‡ä»¶å
        if name in exclude_files:
            continue

        # æ£€æŸ¥æ’é™¤æ¨¡å¼
        excluded = False
        for pattern in exclude_patterns:
            if fnmatch.fnmatch(path, pattern) or fnmatch.fnmatch(name, pattern):
                excluded = True
                break
        if excluded:
            continue

        # URL encode the path to handle special characters
        encoded_path = quote(path, safe='/')
        download_url = f"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{encoded_path}"
        files.append(FileItem(
            path=path,
            name=name,
            sha=item["sha"],
            download_url=download_url,
            size=item.get("size", 0),
        ))

    return files


def fetch_file_list(config: Config) -> list[FileItem]:
    """
    ä» GitHub è·å–æ–‡ä»¶åˆ—è¡¨

    Args:
        config: é…ç½®å¯¹è±¡

    Returns:
        ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶åˆ—è¡¨
    """
    parts = config.github_repo.split("/")
    if len(parts) != 2:
        raise ValueError(f"æ— æ•ˆçš„ä»“åº“æ ¼å¼: {config.github_repo}ï¼Œåº”ä¸º owner/repo")

    owner, repo = parts
    return _fetch_tree_recursive(
        owner=owner,
        repo=repo,
        branch=config.target_branch,
        token=config.github_token,
        exclude_patterns=config.exclude_patterns,
        exclude_files=config.exclude_files,
    )


def download_file(file_item: FileItem, output_dir: Path, token: Optional[str] = None) -> Path:
    """
    ä¸‹è½½å•ä¸ªæ–‡ä»¶

    Args:
        file_item: æ–‡ä»¶æ¡ç›®
        output_dir: è¾“å‡ºç›®å½•
        token: GitHub token (å¯é€‰)

    Returns:
        ä¸‹è½½åçš„æœ¬åœ°è·¯å¾„
    """
    # åˆ›å»ºç›®æ ‡ç›®å½•
    target_path = output_dir / file_item.path
    target_path.parent.mkdir(parents=True, exist_ok=True)

    headers = {"User-Agent": "prolet-tools/1.0"}
    if token:
        headers["Authorization"] = f"Bearer {token}"

    req = Request(file_item.download_url, headers=headers)

    with urlopen(req, timeout=60) as resp:
        content = resp.read()
        target_path.write_bytes(content)

    return target_path


from concurrent.futures import ThreadPoolExecutor, as_completed

def download_all(config: Config, file_list: list[FileItem], output_dir: Path) -> tuple[list[Path], list[str]]:
    """
    æ‰¹é‡ä¸‹è½½æ–‡ä»¶ï¼ˆå¸¦ SHA ç¼“å­˜æœºåˆ¶å’Œå¤šçº¿ç¨‹åŠ é€Ÿï¼‰

    Args:
        config: é…ç½®å¯¹è±¡
        file_list: æ–‡ä»¶åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        (ä¸‹è½½æˆåŠŸçš„æœ¬åœ°æ–‡ä»¶è·¯å¾„åˆ—è¡¨, ä¸‹è½½å¤±è´¥çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨)
    """
    downloaded_paths_map: dict[str, Path] = {}
    failed_files: list[str] = []
    total = len(file_list)
    cache_file = output_dir / "cache.json"
    cache = {}

    # åŠ è½½ç¼“å­˜
    if cache_file.exists():
        try:
            cache = json.loads(cache_file.read_text(encoding="utf-8"))
        except Exception as e:
            print(f"  âš  ç¼“å­˜è¯»å–å¤±è´¥: {e}")

    to_download: list[FileItem] = []
    new_cache = {}
    skipped_count = 0

    # é¢„ç­›é€‰ï¼šåŒºåˆ†ç¼“å­˜æ–‡ä»¶å’Œéœ€è¦ä¸‹è½½çš„æ–‡ä»¶
    for item in file_list:
        target_path = output_dir / item.path
        if target_path.exists() and cache.get(item.path) == item.sha:
            skipped_count += 1
            downloaded_paths_map[item.path] = target_path
            new_cache[item.path] = item.sha
        else:
            to_download.append(item)

    if skipped_count > 0:
        print(f"  âš¡ è·³è¿‡ä¸‹è½½ (å‘½ä¸­ç¼“å­˜): {skipped_count} ä¸ªæ–‡ä»¶")

    if not to_download:
        return [downloaded_paths_map[item.path] for item in file_list], []

    print(f"  ğŸš€ å¼€å§‹å¹¶è¡Œä¸‹è½½ {len(to_download)} ä¸ªæ–°æ–‡ä»¶ (çº¿ç¨‹æ•°: 10)...")
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œä¸‹è½½
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_item = {
            executor.submit(download_file, item, output_dir, config.github_token): item 
            for item in to_download
        }
        
        completed = 0
        for future in as_completed(future_to_item):
            item = future_to_item[future]
            completed += 1
            try:
                path = future.result()
                downloaded_paths_map[item.path] = path
                new_cache[item.path] = item.sha
                if completed % 100 == 0 or completed == len(to_download):
                    print(f"  [{completed}/{len(to_download)}] ä¸‹è½½å®Œæˆ: {item.path}")
            except Exception as e:
                failed_files.append(f"{item.path} ({e})")
                print(f"  âš  ä¸‹è½½å¤±è´¥ ({item.path}): {e}")

    # ä¿å­˜æ–°ç¼“å­˜
    try:
        cache_file.write_text(json.dumps(new_cache, ensure_ascii=False, indent=2), encoding="utf-8")
    except Exception as e:
        print(f"  âš  ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

    # æŒ‰ç…§åŸå§‹åˆ—è¡¨é¡ºåºè¿”å›è·¯å¾„
    successful = [downloaded_paths_map.get(item.path) for item in file_list if item.path in downloaded_paths_map]
    return successful, failed_files
